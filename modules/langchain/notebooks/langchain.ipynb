{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d270dfaf-a322-47a9-9790-8ff2922ae135",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "This notebook has langchain example with Ollama.\n",
    "\n",
    "Find more in this [Ollama Langchain doc](https://python.langchain.com/docs/integrations/llms/ollama/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913a10c7-341e-4185-8fce-47a85e1a8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import time\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e41e3de-1832-4206-9cd6-8cd0da57aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the template of how the LLM should behave\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b62cc7-83ec-444c-b8df-33a73b3c6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the prompt object\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf95317d-8598-4e73-8c90-80102bbf1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusts this variables as needed\n",
    "# base_url=\"http://ollama.tempobr.com\"\n",
    "# base_url=\"http://ollama:11434\"\n",
    "base_url=\"http://ollama-cpu:11434\"\n",
    "model=\"gemma2\"\n",
    "model = OllamaLLM(base_url=base_url, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6292166-6ec7-4395-a354-fbd4b08b35b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "# Before send the request, test if the integration with the Ollama server is running:\n",
    "response = requests.get(base_url)\n",
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5df1aa-a87a-4d50-a145-fc342dc5dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"models\":[{\"name\":\"gemma2:latest\",\"model\":\"gemma2:latest\",\"modified_at\":\"2024-10-28T19:56:03.572045036Z\",\"size\":5443152417,\"digest\":\"ff02c3702f322b9e075e9568332d96c0a7028002f1a5a056e0a6784320a4db0b\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"gemma2\",\"families\":[\"gemma2\"],\"parameter_size\":\"9.2B\",\"quantization_level\":\"Q4_0\"}}]}\n"
     ]
    }
   ],
   "source": [
    "# List all the models on the Ollama server\n",
    "response = requests.get(base_url+\"/api/tags\")\n",
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66fab1e5-8393-427a-aed8-3241f9d09ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prompt on the LLM\n",
    "chain = prompt | model\n",
    "\n",
    "start_time = time.time()\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4c78a8-e73b-4276-8fa1-4e5cb7b65a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 26.047734260559082\n"
     ]
    }
   ],
   "source": [
    "# Calculating the execution time\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution Time:\", execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0ef04-0be0-42b9-91b9-93cb32e7c0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
