{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d270dfaf-a322-47a9-9790-8ff2922ae135",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "This notebook has RAG example with Ollama.\n",
    "\n",
    "Following this tutorial: [Build a Retrieval Augmented Generation (RAG) App\n",
    "](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bdc4d-c9db-4dcc-8cfd-292097a800ba",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing and inserting appropriate information into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally.\n",
    "\n",
    "[reference](https://python.langchain.com/docs/tutorials/rag/#what-is-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a36cc-112c-4761-bc1b-557598021445",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913a10c7-341e-4185-8fce-47a85e1a8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import time\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e41e3de-1832-4206-9cd6-8cd0da57aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the template of how the LLM should behave\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b62cc7-83ec-444c-b8df-33a73b3c6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the prompt object\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf95317d-8598-4e73-8c90-80102bbf1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusts this variables as needed\n",
    "# base_url=\"http://ollama.tempobr.com\"\n",
    "# base_url=\"http://ollama:11434\"\n",
    "base_url=\"http://ollama-cpu:11434\"\n",
    "model=\"gemma2\"\n",
    "model = OllamaLLM(base_url=base_url, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6292166-6ec7-4395-a354-fbd4b08b35b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "# Before send the request, test if the integration with the Ollama server is running:\n",
    "response = requests.get(base_url)\n",
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d5df1aa-a87a-4d50-a145-fc342dc5dbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"models\":[{\"name\":\"gemma2:latest\",\"model\":\"gemma2:latest\",\"modified_at\":\"2024-10-30T21:42:24.954979883Z\",\"size\":5443152417,\"digest\":\"ff02c3702f322b9e075e9568332d96c0a7028002f1a5a056e0a6784320a4db0b\",\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"gemma2\",\"families\":[\"gemma2\"],\"parameter_size\":\"9.2B\",\"quantization_level\":\"Q4_0\"}}]}\n"
     ]
    }
   ],
   "source": [
    "# List all the models on the Ollama server\n",
    "response = requests.get(base_url+\"/api/tags\")\n",
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66fab1e5-8393-427a-aed8-3241f9d09ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the prompt on the LLM\n",
    "chain = prompt | model\n",
    "\n",
    "start_time = time.time()\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4c78a8-e73b-4276-8fa1-4e5cb7b65a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 41.726380348205566\n"
     ]
    }
   ],
   "source": [
    "# Calculating the execution time\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"Execution Time:\", execution_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
