{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d270dfaf-a322-47a9-9790-8ff2922ae135",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "This notebook has RAG example with Ollama.\n",
    "\n",
    "Following this tutorial: [Build a Retrieval Augmented Generation (RAG) App\n",
    "](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bdc4d-c9db-4dcc-8cfd-292097a800ba",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing and inserting appropriate information into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally.\n",
    "\n",
    "[reference](https://python.langchain.com/docs/tutorials/rag/#what-is-rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a36cc-112c-4761-bc1b-557598021445",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "![rag_indexing](images/rag_indexing.png)\n",
    "[reference](https://python.langchain.com/docs/tutorials/rag/#indexing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c056ae-acc1-4031-9b3a-ddbe4b53a577",
   "metadata": {},
   "source": [
    "## RAG steps\n",
    "\n",
    "1. **Load**: First we need to load our data. This is done with Document Loaders.   \n",
    "2. **Split**: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. **Embed**: The embbedings were generated and are ready to be stored.\n",
    "4. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\n",
    "5. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
    "6. **Generate**: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d87a0-e381-43d5-8396-84c403f1ea46",
   "metadata": {},
   "source": [
    "## Retrieval and generation\n",
    "\n",
    "![rag_retrieval_generation](images/rag_retrieval_generation.png)\n",
    "[reference](https://python.langchain.com/docs/tutorials/rag/#retrieval-and-generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf3ee8-a170-4fdc-8ba0-d5714bed9684",
   "metadata": {},
   "source": [
    "## Building the example\n",
    "\n",
    "The Langchain tutorial, asks to use a \"LLM service\" like openai. In this example, i'll use my local Ollama server with local LLMs.\n",
    "\n",
    "To do that I used this tutorial [OllamaEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/ollama/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cae6ab9-94cd-46ff-afad-d408b1efedf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing packages\n",
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma\n",
    "# Installing Ollama embeddings\n",
    "%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df4043b-bb67-4cea-a6d8-16efbe8cdb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# Ollama local imports\n",
    "import requests\n",
    "import time\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Ollama embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# Adjusts this variables as needed\n",
    "# base_url=\"http://ollama.tempobr.com\"\n",
    "# base_url=\"http://ollama:11434\"\n",
    "base_url=\"http://ollama-cpu:11434\"\n",
    "model=\"llama3.2\"\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=base_url,\n",
    "    model=model,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2931c4af-8a29-47c4-996a-a1b9dc16cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store with a sample text\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "text = \"LangChain is the framework for building context-aware reasoning applications\"\n",
    "\n",
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text],\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6929afd-fda4-4311-9af3-1bf160192ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is the framework for building context-aware reasoning applications'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "# show the retrieved document's content\n",
    "retrieved_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b6878b-a79a-4e7a-b4b1-8607ba72634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.012316747, 0.0007064336, 0.016245157, -0.020877853, -0.0022321339, -0.011591902, -0.011317743, 0\n"
     ]
    }
   ],
   "source": [
    "single_vector = embeddings.embed_query(text)\n",
    "print(str(single_vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777f0c93-848d-4842-b68f-56d593d9f503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.012316747, 0.0007064336, 0.016245157, -0.020877853, -0.0022321339, -0.011591902, -0.011317743, 0\n",
      "[-0.004660674, 0.0026602747, 0.004631577, -0.0038133785, -0.0017676974, -0.017241802, -0.0041689477,\n"
     ]
    }
   ],
   "source": [
    "text2 = (\n",
    "    \"LangGraph is a library for building stateful, multi-actor applications with LLMs\"\n",
    ")\n",
    "two_vectors = embeddings.embed_documents([text, text2])\n",
    "for vector in two_vectors:\n",
    "    print(str(vector)[:100])  # Show the first 100 characters of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16bcd34-8e81-48f7-a047-3c3525de5f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='392596b3-8aad-45cc-a9c3-5cf873343375', metadata={}, page_content='LangChain is the framework for building context-aware reasoning applications'),\n",
       " Document(id='976cea32-8d41-4dc5-a728-5082ec6b55e5', metadata={}, page_content='LangGraph is a library for building stateful, multi-actor applications with LLMs')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = InMemoryVectorStore.from_texts(\n",
    "    [text, text2],\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retrieved_documents = retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "retrieved_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e41e3de-1832-4206-9cd6-8cd0da57aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the template of how the LLM should behave\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer: Answer the question according to the context.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b62cc7-83ec-444c-b8df-33a73b3c6bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the prompt object\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf95317d-8598-4e73-8c90-80102bbf1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaLLM(base_url=base_url, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6292166-6ec7-4395-a354-fbd4b08b35b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "# Before send the request, test if the integration with the Ollama server is running:\n",
    "response = requests.get(base_url)\n",
    "if response.status_code == 200:\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58c99d17-c882-4026-8d81-6897073cda4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is the framework for building context-aware reasoning applications\n",
      "LangGraph is a library for building stateful, multi-actor applications with LLMs\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\".join([doc.page_content for doc in retrieved_documents])\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fab1e5-8393-427a-aed8-3241f9d09ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source Python framework designed specifically for building context-aware reasoning applications that leverage large language models (LLMs) and graph neural networks. It enables developers to easily integrate various libraries such as Hugging Face's Transformers and PyTorch with their own custom logic, allowing them to build more sophisticated AI models tailored to specific domain requirements.\n",
      "Execution Time: 8.75894021987915\n"
     ]
    }
   ],
   "source": [
    "# Run the prompt on the LLM\n",
    "chain = prompt | model\n",
    "\n",
    "start_time = time.time()\n",
    "response = chain.invoke({\"question\": \"What is LangChain?\", \"context\": context})\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(response)\n",
    "print(\"Execution Time:\", execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f4c78a8-e73b-4276-8fa1-4e5cb7b65a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph is a library used in conjunction with LangChain, which enables developers to build stateful, multi-actor applications that leverage Large Language Models (LLMs). This allows for more complex and dynamic reasoning capabilities within the framework of LangChain.\n",
      "Execution Time: 6.536516189575195\n"
     ]
    }
   ],
   "source": [
    "# Run the prompt on the LLM\n",
    "chain = prompt | model\n",
    "\n",
    "start_time = time.time()\n",
    "response = chain.invoke({\"question\": \"What is LangGraph?\", \"context\": context})\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(response)\n",
    "print(\"Execution Time:\", execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be0d8a-1609-42be-b287-a06536719c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
